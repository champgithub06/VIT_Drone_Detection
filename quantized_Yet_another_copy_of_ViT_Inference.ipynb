{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#inference"
      ],
      "metadata": {
        "id": "9L7hl3dduB8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow -q\n",
        "!pip install tensorflow-addons -q\n",
        "!pip install opencv-python -q\n",
        "!pip install scikit-learn -q\n",
        "!pip install keras -q"
      ],
      "metadata": {
        "id": "ghbSNYi5B4aI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df3abfb6-0c19-4364-ea79-1570f838bd46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-addons (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-addons\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import keras\n",
        "from PIL import Image\n",
        "from keras import layers\n",
        "from scipy.special import erf # Importing erf function"
      ],
      "metadata": {
        "id": "5by0gtUsCEzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbJFA84MDWCO",
        "outputId": "25a992e6-8c8a-4052-b462-ba5af131fa35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Image Params\n",
        "patch_height   = 8\n",
        "patch_width    = 8\n",
        "img_height     = 128\n",
        "img_width      = 128\n",
        "scaling_factor = 2**10\n",
        "mha_scaling_factor = 2**10\n",
        "\n",
        "# Path for Image Source and Parameter\n",
        "#dataset_path   = \"/content/drive/MyDrive/dataset_vision_trands/ExeImg_Dataset/Malware/956dde17805ed5de07358ebed7656536_0.jpg\"\n",
        "#dataset_path   = \"/content/drive/MyDrive/dataset_vision_trands/ExeImg_Dataset/Benignware/fce1e7e45cc768dbd55669979806374c_0.jpg\"  57%\n",
        "#dataset_path   = \"/content/drive/MyDrive/dataset_vision_trands/ExeImg_Dataset/Benignware/fe63a1786918eb5faf5f0d96730a6068_2.jpg\"  55%\n",
        "#dataset_path   = \"/content/drive/MyDrive/dataset_vision_trands/ExeImg_Dataset/Benignware/fe3683b683010e2361cc0ffc6aa82155_1.jpg\"    60%\n",
        "#dataset_path   = \"/content/drive/MyDrive/dataset_vision_trands/ExeImg_Dataset/Benignware/fc0b78df7af26182394c6f5e15f0b133_0.jpg\"    60%\n",
        "#dataset_path   = \"/content/drive/MyDrive/dataset_vision_trands/ExeImg_Dataset/Benignware/f935d1f09f78044f1cdfa91c07016746_1.jpg\"   59.9%\n",
        "dataset_path    =\"/content/drive/MyDrive/drone_vit/drone_dataset/no drone/00000463_(5).jpg\"  #61.7 %\n",
        "\n",
        "# Layer Output\n",
        "lp_infered_path  = \"/content/drive/MyDrive/drone_vit/Outputs/layers_vit_model_dlt/patch_encoder_3.txt\"\n",
        "ln1_infered_path = \"/content/drive/MyDrive/drone_vit/Outputs/layers_vit_model_dlt/layer_normalization_9.txt\"\n",
        "mha_infered_path = \"/content/drive/MyDrive/drone_vit/Outputs/layers_vit_model_dlt/multi_head_attention_3.txt\"\n",
        "add1_infered_path = \"/content/drive/MyDrive/drone_vit/Outputs/layers_vit_model_dlt/add_6.txt\"\n",
        "ln2_infered_path  =  \"/content/drive/MyDrive/drone_vit/Outputs/layers_vit_model_dlt/layer_normalization_10.txt\"\n",
        "mlp_infered_path  =  \"/content/drive/MyDrive/drone_vit/Outputs/layers_vit_model_dlt/patches_3.txt\"\n",
        "add2_infered_path = \"/content/drive/MyDrive/drone_vit/Outputs/layers_vit_model_dlt/add_7.txt\"\n",
        "ln3_infered_path  =  \"/content/drive/MyDrive/drone_vit/Outputs/layers_vit_model_dlt/layer_normalization_11.txt\"\n",
        "fc_infered_path  =  \"/content/drive/MyDrive/drone_vit/Outputs/layers_vit_model_dlt/flatten_3.txt\"\n",
        "\n",
        "# lp_scaled_infered_path   = \"/content/drive/MyDrive/drone_vit/Outputs/layers_vit_model_dlt/patch_encoder_3_s.txt\"\n",
        "# ln1_scaled_infered_path  = \"/content/drive/MyDrive/drone_vit/Outputs/layers_vit_model_dlt/layer_normalization_9_s.txt\"\n",
        "# mha_scaled_infered_path  = \"/content/drive/MyDrive/drone_vit/Outputs/layers_vit_model_dlt/multi_head_attention_3_s.txt\"\n",
        "# add1_scaled_infered_path = \"/content/drive/MyDrive/drone_vit/Outputs/layers_vit_model_dlt/add_6_s.txt\"\n",
        "# ln2_scaled_infered_path  =  \"/content/drive/MyDrive/drone_vit/Outputs/layers_vit_model_dlt/layer_normalization_10_s.txt\"\n",
        "# mlp_scaled_infered_path  =  \"/content/drive/MyDrive/drone_vit/Outputs/layers_vit_model_dlt/patches_3_s.txt\"\n",
        "# add2_scaled_infered_path = \"/content/drive/MyDrive/drone_vit/Outputs/layers_vit_model_dlt/add_7_s.txt\"\n",
        "# ln3_scaled_infered_path  =  \"/content/drive/MyDrive/drone_vit/Outputs/layers_vit_model_dlt/layer_normalization_11_s.txt\"\n",
        "# fc_scaled_infered_path   =  \"/content/drive/MyDrive/drone_vit/Outputs/layers_vit_model_dlt/flatten_3_s.txt\"\n",
        "\n",
        "# Weights\n",
        "lp_weight_path = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/lp_weight.txt\"\n",
        "lp_bias_path   = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/lp_bias.txt\"\n",
        "lp_emb_path    = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/lp_pos_emb.txt\"\n",
        "mha_wq0_path   = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/mha_weight_q0.txt\"\n",
        "mha_wk0_path   = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/mha_weight_k0.txt\"\n",
        "mha_wv0_path   = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/mha_weight_v0.txt\"\n",
        "mha_wq1_path   = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/mha_weight_q1.txt\"\n",
        "mha_wk1_path   = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/mha_weight_k1.txt\"\n",
        "mha_wv1_path   = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/mha_weight_v1.txt\"\n",
        "mha_wqkv_path  = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/mha_weight_qkv.txt\"\n",
        "mha_wouput_path   = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/mha_weight_last.txt\"\n",
        "mlp_d0_w_path  = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/mlp_weight0.txt\"\n",
        "mlp_d0_b_path  = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/mlp_bias0.txt\"\n",
        "mlp_d1_w_path  = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/mlp_weight1.txt\"\n",
        "mlp_d1_b_path  = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/mlp_bias1.txt\"\n",
        "fc_d0_w_path  = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/fc_weight0.txt\"\n",
        "fc_d0_b_path  = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/fc_bias0.txt\"\n",
        "fc_d1_w_path  = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/fc_weight1.txt\"\n",
        "fc_d1_b_path  = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/fc_bias1.txt\"\n",
        "fc_d2_w_path  = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/fc_weight2.txt\"\n",
        "fc_d2_b_path  = \"/content/drive/MyDrive/drone_vit/Outputs/weights_vit_model_dlt/fc_bias2.txt\"\n",
        "\n",
        "# COE\n",
        "# position_emb_coe_path = \"/content/drive/MyDrive/dataset_vision_trands/ViT/Outputs/lp/position_emb_coe_path.coe\"\n",
        "# lp_coe_path = \"/content/drive/MyDrive/dataset_vision_trands/ViT/Outputs/lp/lp_coe_weight.coe\"\n",
        "# bias_coe_path =\"/content/drive/MyDrive/dataset_vision_trands/ViT/Outputs/lp/lp_coe_bias.coe \"\n",
        "#mha_wqkv_comb_coe_path = \"/content/drive/MyDrive/Project/ViT/Outputs/mha/mha_wqkv_comb.coe\"\n",
        "#mha_wq0_coe_path = \"/content/drive/MyDrive/Project/ViT/Outputs/mha/weight_wq0.coe\"\n",
        "#mha_wk0_coe_path = \"/content/drive/MyDrive/Project/ViT/Outputs/mha/weight_wk0.coe\"\n",
        "#mha_wv0_coe_path = \"/content/drive/MyDrive/Project/ViT/Outputs/mha/weight_wv0.coe\"\n",
        "#mha_wq1_coe_path = \"/content/drive/MyDrive/Project/ViT/Outputs/mha/weight_wq1.coe\"\n",
        "#mha_wk1_coe_path = \"/content/drive/MyDrive/Project/ViT/Outputs/mha/weight_wk1.coe\"\n",
        "#mha_wv1_coe_path = \"/content/drive/MyDrive/Project/ViT/Outputs/mha/weight_wv1.coe\"\n",
        "#mha_wo_coe_path = \"/content/drive/MyDrive/Project/ViT/Outputs/mha/weight_wo.coe\"\n",
        "\n",
        "# # Scaled Output\n",
        "# ln1_scaled_path = \"/content/drive/MyDrive/dataset_vision_trands/ViT/Outputs/ln1/ln1_data_o.txt\"\n"
      ],
      "metadata": {
        "id": "k_R1OuPihGhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # # Augmentaion\n",
        "  data_augmentation = keras.Sequential([\n",
        "      layers.Resizing(img_width, img_height)\n",
        "  ])"
      ],
      "metadata": {
        "id": "TErfZ4zyrkMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GELU\n",
        "def gelu_exact(x):\n",
        "    return 0.5 * x * (1 + erf(x / np.sqrt(2)))\n"
      ],
      "metadata": {
        "id": "hh3yIRg9nNU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and rounding\n",
        "def scaled_int(input_arry):\n",
        "  float_array = np.array(input_arry)\n",
        "  input_scaled = np.round(float_array * scaling_factor).astype(int)\n",
        "  return input_scaled"
      ],
      "metadata": {
        "id": "uTqYJha1nP77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to 2's complement for an array\n",
        "def twos_complement(num_array, bits):\n",
        "    hex_array = []\n",
        "    np_arry = np.array(num_array).flatten()\n",
        "    for num in np_arry:\n",
        "        if num < 0:\n",
        "            num = (1 << bits) + num  # Apply 2's complement\n",
        "        hex_value = hex(num)[2:].zfill(bits // 4).upper()  # Format to hex and pad\n",
        "        hex_array.append(hex_value)\n",
        "    return np.array(hex_array)"
      ],
      "metadata": {
        "id": "jyaAqAdQnTR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Patching\n",
        "def patch(img,img_width,img_height,patch_width,patch_height):\n",
        "  patches   = []\n",
        "  patch_cnt = 0\n",
        "  # Plotter\n",
        "\n",
        "  for height_itr in range(0, img_height, patch_height):  # Loop over rows\n",
        "      for width_itr in range(0, img_width, patch_width):  # Loop over columns\n",
        "          patch = img[height_itr:height_itr+patch_height, width_itr:width_itr+patch_width]\n",
        "          patches.append(patch)\n",
        "          patch_cnt = patch_cnt + 1;\n",
        "\n",
        "  return patches"
      ],
      "metadata": {
        "id": "FWL21fEKCKcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Projection\n",
        "def lp(data,patches_num,patch_width,patch_height,scaled_output):\n",
        "\n",
        "  weight  = np.loadtxt(lp_weight_path, delimiter=\",\").reshape(192,64)\n",
        "  bias    = np.loadtxt(lp_bias_path, delimiter=\",\").reshape(1,64)\n",
        "  pos_emb = np.loadtxt(lp_emb_path, delimiter=\",\").reshape(256,64)\n",
        "\n",
        "  weight_scaled  = np.round(weight * scaling_factor).astype(int)\n",
        "  bias_scaled    = np.round(bias * scaling_factor).astype(int)\n",
        "  pos_emb_scaled = np.round(pos_emb * scaling_factor).astype(int)\n",
        "\n",
        "  embed_img   = []\n",
        "  embed_img_s = []\n",
        "\n",
        "  # Flattening\n",
        "  for patch_count in range(0,patches_num):\n",
        "    flattened_patch = data[patch_count].reshape(1, 192)\n",
        "\n",
        "    # Weight Multiplication\n",
        "    weighted_patch    = np.dot(flattened_patch,weight)\n",
        "    weighted_patch_s  = np.dot(flattened_patch,weight_scaled)\n",
        "    weighted_patch_ds = weighted_patch_s\n",
        "\n",
        "    # Bias addition\n",
        "    biased_patch   = weighted_patch + bias\n",
        "    biased_patch_s = weighted_patch_ds + bias_scaled\n",
        "\n",
        "    # Patch Embedding\n",
        "    embed_patch   = biased_patch + pos_emb[patch_count]\n",
        "    embed_patch_s = biased_patch_s + pos_emb_scaled[patch_count]\n",
        "\n",
        "    # Image Generation\n",
        "    embed_img.append(embed_patch )\n",
        "    embed_img_s.append(embed_patch_s)\n",
        "\n",
        "  if(scaled_output == 1):\n",
        "    return embed_img_s\n",
        "  else:\n",
        "    return embed_img"
      ],
      "metadata": {
        "id": "TgrdZLVRngQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Layer Normalization 1\n",
        "def ln1(data,patches_num,patch_width,patch_height,scaled_output):\n",
        "\n",
        "  ln_array = []\n",
        "  ln_scaled_array = []\n",
        "\n",
        "  for patch_count in range(0,patches_num):\n",
        "    # Row Extraction\n",
        "    row_array = data[patch_count]\n",
        "    # Mean\n",
        "    mean      = np.mean(data[patch_count])\n",
        "    # Standard Deviation\n",
        "    stdev     = np.std(row_array, ddof=1)\n",
        "\n",
        "    # Normilization\n",
        "    normalized_array = (row_array - mean) / stdev\n",
        "\n",
        "    # Scaled\n",
        "    normalized_array_scaled = (row_array - int(mean)) * 256 // int(stdev)\n",
        "    int_arry = (normalized_array_scaled).astype(int)\n",
        "    ln_scaled_array.append(int_arry)\n",
        "    ln_array.append(normalized_array)\n",
        "\n",
        "  if(scaled_output):\n",
        "    return ln_scaled_array\n",
        "  else:\n",
        "    return ln_array"
      ],
      "metadata": {
        "id": "RN0kdZtSnkBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi Layer Attention\n",
        "def softmax(x,scaled_output):\n",
        "    if(scaled_output==1):\n",
        "      exp_x = np.exp(x/scaling_factor - np.max(x/scaling_factor, axis=-1, keepdims=True))  # Stable softmax\n",
        "      return (exp_x / np.sum(exp_x, axis=-1, keepdims=True)) * scaling_factor\n",
        "    else:\n",
        "       exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # Stable softmax\n",
        "       return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "def mha(data,patches_num,patch_width,patch_height,scaled_output):\n",
        "\n",
        "  # Weights 1\n",
        "  wq0 = np.loadtxt(mha_wq0_path, delimiter=\",\").reshape(64,32)\n",
        "  wk0 = np.loadtxt(mha_wk0_path, delimiter=\",\").reshape(64,32)\n",
        "  wv0 = np.loadtxt(mha_wv0_path, delimiter=\",\").reshape(64,32)\n",
        "\n",
        "  # Weights 2\n",
        "  wq1 = np.loadtxt(mha_wq1_path, delimiter=\",\").reshape(64,32)\n",
        "  wk1 = np.loadtxt(mha_wk1_path, delimiter=\",\").reshape(64,32)\n",
        "  wv1 = np.loadtxt(mha_wv1_path, delimiter=\",\").reshape(64,32)\n",
        "  # Weight output\n",
        "  w0u = np.loadtxt(mha_wouput_path, delimiter=\",\").reshape(64,64)\n",
        "\n",
        "  wq0_scaled =  np.round(wq0 * scaling_factor).astype(int)\n",
        "  wk0_scaled =  np.round(wk0 * scaling_factor).astype(int)\n",
        "  wv0_scaled =  np.round(wv0 * scaling_factor).astype(int)\n",
        "\n",
        "  # Weights 2\n",
        "  wq1_scaled =  np.round(wq1 * scaling_factor).astype(int)\n",
        "  wk1_scaled =  np.round(wk1 * scaling_factor).astype(int)\n",
        "  wv1_scaled =  np.round(wv1 * scaling_factor).astype(int)\n",
        "  # Weight output\n",
        "  w0u_scaled =  np.round(w0u * scaling_factor).astype(int)\n",
        "\n",
        "\n",
        "  mha_array = []\n",
        "  q0_list   = []\n",
        "  k0_list   = []\n",
        "  v0_list   = []\n",
        "  q1_list   = []\n",
        "  k1_list   = []\n",
        "  v1_list   = []\n",
        "  mha_scaled_array  = []\n",
        "  q0_scaled_list   = []\n",
        "  k0_scaled_list   = []\n",
        "  v0_scaled_list   = []\n",
        "  q1_scaled_list   = []\n",
        "  k1_scaled_list   = []\n",
        "  v1_scaled_list   = []\n",
        "  for patch_count in range(0,patches_num):\n",
        "    patch = data[patch_count]  # Extract patch\n",
        "    #print(\"patch\",patch[0][0])\n",
        "    # Compute Q, K, V for both heads\n",
        "    q0, k0, v0 = np.dot(patch, wq0), np.dot(patch, wk0), np.dot(patch, wv0)\n",
        "    q1, k1, v1 = np.dot(patch, wq1), np.dot(patch, wk1), np.dot(patch, wv1)\n",
        "    #print(\"q0\",q0[0][0])\n",
        "    q0_list.append(q0)\n",
        "    k0_list.append(k0)\n",
        "    v0_list.append(v0)\n",
        "    q1_list.append(q1)\n",
        "    k1_list.append(k1)\n",
        "    v1_list.append(v1)\n",
        "\n",
        "    q0_scaled, k0_scaled, v0_scaled = np.dot(patch, wq0_scaled).astype(int), np.dot(patch, wk0_scaled).astype(int), np.dot(patch, wv0_scaled).astype(int)\n",
        "    q1_scaled, k1_scaled, v1_scaled = np.dot(patch, wq1_scaled).astype(int), np.dot(patch, wk1_scaled).astype(int), np.dot(patch, wv1_scaled).astype(int)\n",
        "\n",
        "    q0_scaled_list.append(q0_scaled)\n",
        "    k0_scaled_list.append(k0_scaled)\n",
        "    v0_scaled_list.append(v0_scaled)\n",
        "    q1_scaled_list.append(q1_scaled)\n",
        "    k1_scaled_list.append(k1_scaled)\n",
        "    v1_scaled_list.append(v1_scaled)\n",
        "\n",
        "  q0_matrix = np.vstack(q0_list)\n",
        "  k0_matrix = np.vstack(k0_list)\n",
        "  v0_matrix = np.vstack(v0_list)\n",
        "  q1_matrix = np.vstack(q1_list)\n",
        "  k1_matrix = np.vstack(k1_list)\n",
        "  v1_matrix = np.vstack(v1_list)\n",
        "\n",
        "  q0_scaled_matrix = np.vstack(q0_scaled_list)\n",
        "  k0_scaled_matrix = np.vstack(k0_scaled_list)\n",
        "  v0_scaled_matrix = np.vstack(v0_scaled_list)\n",
        "  q1_scaled_matrix = np.vstack(q1_scaled_list)\n",
        "  k1_scaled_matrix = np.vstack(k1_scaled_list)\n",
        "  v1_scaled_matrix = np.vstack(v1_scaled_list)\n",
        "  #print(\"q0_scaled_matrix\",q0_scaled_matrix[0][0])\n",
        "  #print(\"k0_scaled_matrix\",k0_scaled_matrix[0][0])\n",
        "  #print(\"v0_scaled_matrix\",v0_scaled_matrix[0][0])\n",
        "  #print(\"q0_matrix\",q0_matrix[0][0])\n",
        "  #print(\"k0_matrix\",k0_matrix[0][0])\n",
        "  #print(\"v0_matrix\",v0_matrix[0][0])\n",
        "\n",
        "  # # Scaled Dot-Product Attention\n",
        " # qkv0_mat = np.dot(softmax(np.dot(q0_matrix, k0_matrix.T) / np.sqrt(64),0), v0_matrix)\n",
        "  #qkv1_mat = np.dot(softmax(np.dot(q1_matrix, k1_matrix.T) / np.sqrt(64)), v1_matrix)\n",
        "  qkv0_mat = np.dot(softmax(np.dot(q0_matrix, k0_matrix.T) / np.sqrt(32),0), v0_matrix)\n",
        "  qkv1_mat = np.dot(softmax(np.dot(q1_matrix, k1_matrix.T) / np.sqrt(32),0), v1_matrix)\n",
        "  #print(qkv0_mat.shape)\n",
        "  #print(\"qkv0_mat\",qkv0_mat[0][1])\n",
        "  #print(qkv1_mat.shape)\n",
        "  #print(\"qkv1_mat\",qkv1_mat[0][1])\n",
        "\n",
        "\n",
        "  #qkv0_scaled_mat = np.dot((np.dot(q0_scaled_matrix  , k0_scaled_matrix.T ) // np.sqrt(64)), v0_scaled_matrix) / scaling_factor\n",
        "  #qkv1_scaled_mat = np.dot((np.dot(q1_scaled_matrix , k1_scaled_matrix.T  ) //  np.sqrt(64)), v1_scaled_matrix )/ scaling_factor\n",
        "  qkv0_scaled_mat = np.dot(softmax(np.dot(q0_scaled_matrix  , k0_scaled_matrix.T ) // (np.sqrt(32)),1), v0_scaled_matrix) / scaling_factor\n",
        "  qkv1_scaled_mat = np.dot(softmax(np.dot(q1_scaled_matrix , k1_scaled_matrix.T  ) // ( np.sqrt(32)),1), v1_scaled_matrix )/ scaling_factor\n",
        "  #print( qkv0_scaled_mat.shape)\n",
        "  #print ( \"qkv0_scaled_mat\",qkv0_scaled_mat[0][1])\n",
        "  #print(qkv1_mat.shape)\n",
        "  #print( \"qkv1_scaled_mat\",qkv1_scaled_mat[0][1])\n",
        "\n",
        "  # tuple(int(x) for x in qkv0_scaled_mat)\n",
        "\n",
        "  # # if(patch_count == 0):\n",
        "  # #   print(qkv0_mat.shape)\n",
        "  #qkvo0_mat = np.dot(qkv0_mat,wo0)\n",
        "  #qkvo1_mat = np.dot(qkv0_mat,wo1)\n",
        "  qkv_combined = np.hstack((qkv0_mat,qkv1_mat ))\n",
        "  qkv_scaled_combined = np.hstack((qkv0_scaled_mat,qkv1_scaled_mat ))\n",
        "  #qkv_combined = np.hstack((qkvo0_mat,qkvo1_mat ))\n",
        "  # ## Compute output for attenion\n",
        "  qkv_final = np.dot(qkv_combined, w0u)\n",
        "  qkv_scaled_final = np.dot(qkv_scaled_combined,   w0u_scaled)\n",
        "  int_arry = qkv_scaled_final.astype(int)\n",
        "  mha_array.append(qkv_final)\n",
        "  mha_scaled_array.append(int_arry)\n",
        "\n",
        "  # # print(qkv_combined.shape)\n",
        "  if(scaled_output == 1):\n",
        "   # print( mha_scaled_array)\n",
        "    return mha_scaled_array\n",
        "  else:\n",
        "    #print(mha_array)\n",
        "    return mha_array\n"
      ],
      "metadata": {
        "id": "Eb9Qvl7Nxzl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add(input1,input2,rows):\n",
        "  add_arry = []\n",
        "  # Perform element-wise addition\n",
        "  for i in range(0,rows):\n",
        "    input1_row = input1[i]\n",
        "    input2_row = input2[i]\n",
        "    add_res = np.add(input1_row,input2_row)\n",
        "    add_arry.append(add_res)\n",
        "    #print(add_1.shape)\n",
        "  return add_arry"
      ],
      "metadata": {
        "id": "a7wbqY9Hp8Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Layer Normalization 1\n",
        "def ln2(data,patches_num,patch_width,patch_height,scaled_output):\n",
        "\n",
        "  ln2_array = []\n",
        "  ln2_scaled_array = []\n",
        "\n",
        "  for patch_count in range(0,patches_num):\n",
        "    # Row Extraction\n",
        "    row_array = data[patch_count]\n",
        "    # Mean\n",
        "    mean      = np.mean(data[patch_count])\n",
        "    # Standard Deviation\n",
        "    stdev     = np.std(row_array, ddof=1)\n",
        "\n",
        "    # Normilization\n",
        "    normalized_array = (row_array - mean) / stdev\n",
        "\n",
        "    # Scaled\n",
        "    normalized_array_scaled = (row_array - int(mean)) * 256 // int(stdev)\n",
        "    int_arry = (normalized_array_scaled).astype(int)\n",
        "    ln2_scaled_array.append(int_arry)\n",
        "    ln2_array.append(normalized_array)\n",
        "\n",
        "  if(scaled_output):\n",
        "    return ln2_scaled_array\n",
        "  else:\n",
        "    return ln2_array"
      ],
      "metadata": {
        "id": "BzV2VMpxil4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP\n",
        "def mlp(data):\n",
        "\n",
        "  w_dense0 = np.loadtxt(mlp_d0_w_path, delimiter=\",\").reshape(64,128)\n",
        "  b_dense0 = np.loadtxt(mlp_d0_b_path, delimiter=\",\").reshape(1,128)\n",
        "  w_dense1 = np.loadtxt(mlp_d1_w_path, delimiter=\",\").reshape(128,64)\n",
        "  b_dense1 = np.loadtxt(mlp_d1_b_path, delimiter=\",\").reshape(1,64)\n",
        "\n",
        "  data_np_arry = np.array(data)\n",
        "  mlp_array = []\n",
        "\n",
        "  # Dense0\n",
        "  dense0 = np.dot(data_np_arry,w_dense0) + b_dense0 #256x128\n",
        "  dense0_act = gelu_exact(dense0)\n",
        "\n",
        "  # Dense1\n",
        "  dense1 = np.dot(dense0_act,w_dense1) + b_dense1\n",
        "  dense1_act = gelu_exact(dense1)\n",
        "\n",
        "  mlp_array.append(dense1_act)\n",
        "\n",
        "  return mlp_array"
      ],
      "metadata": {
        "id": "rOwB1rPDqEQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fully Connected\n",
        "def fc(data):\n",
        "\n",
        "    # Flattening\n",
        "    data_np_arry = np.array(data).reshape(1,16384)\n",
        "    #print(data_np_arry.shape)\n",
        "\n",
        "    # These reshape values are now updated to match your [2048, 1024] model\n",
        "    w_dense0 = np.loadtxt(fc_d0_w_path, delimiter=\",\").reshape(16384, 2048) # CHANGED\n",
        "    b_dense0 = np.loadtxt(fc_d0_b_path, delimiter=\",\").reshape(1, 2048)     # CHANGED\n",
        "    w_dense1 = np.loadtxt(fc_d1_w_path, delimiter=\",\").reshape(2048, 1024)   # CHANGED\n",
        "    b_dense1 = np.loadtxt(fc_d1_b_path, delimiter=\",\").reshape(1, 1024)     # CHANGED\n",
        "    w_dense2 = np.loadtxt(fc_d2_w_path, delimiter=\",\").reshape(1024, 2)     # CHANGED\n",
        "    b_dense2 = np.loadtxt(fc_d2_b_path, delimiter=\",\").reshape(1, 2)        # Stays the same\n",
        "\n",
        "    fc_array = []\n",
        "\n",
        "    # Dense0\n",
        "    dense0 = np.dot(data_np_arry,w_dense0) + b_dense0\n",
        "    #print(\"dense0\",dense0[0][0])\n",
        "    dense0_act = gelu_exact(dense0)\n",
        "\n",
        "    # Dense1\n",
        "    dense1 = np.dot(dense0_act,w_dense1) + b_dense1\n",
        "    dense1_act = gelu_exact(dense1)\n",
        "\n",
        "    # Dense2 (Final Output)\n",
        "    dense2 = np.dot(dense1_act,w_dense2) + b_dense2\n",
        "    #dense2 = np.dot(dense1_act,w_dense2)\n",
        "    print(\"dense2 \",dense2[0])\n",
        "\n",
        "\n",
        "    fc_array.append(dense2)\n",
        "\n",
        "    return fc_array"
      ],
      "metadata": {
        "id": "LrAwoMteqIs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vision Transfomer\n",
        "\n",
        "# Inference\n",
        "def vit_infer():\n",
        "  # Reading Image\n",
        "  img_data   = Image.open(dataset_path)\n",
        "\n",
        "  # Augmentation\n",
        "  augmented = data_augmentation(np.array(img_data))\n",
        "\n",
        "  # Convert to NumPy array\n",
        "  aug_img      = np.array(augmented).astype(\"uint8\")\n",
        "\n",
        "  # Patching\n",
        "  patches_num = (img_height // patch_height) * (img_width // patch_width)\n",
        "  patches     = patch(aug_img,img_width,img_height,patch_width,patch_height)\n",
        "\n",
        "  # Linear Projection\n",
        "  lp_array        = lp(patches,patches_num,patch_width,patch_height,0)\n",
        "  lp_array_scaled = lp(patches,patches_num,patch_width,patch_height,1)\n",
        "  np.savetxt(lp_infered_path, np.array(lp_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "  np.savetxt(lp_scaled_infered_path, np.array(lp_array_scaled).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "\n",
        "  # Layer Normalization\n",
        "  ln1_array = ln1(lp_array,patches_num,patch_width,patch_height,0)\n",
        "  ln1_scaled_array = ln1(lp_array_scaled,patches_num,patch_width,patch_height,1)\n",
        "  # np.savetxt(ln1_infered_path, np.array(ln1_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "  # np.savetxt(ln1_scaled_infered_path, np.array(ln1_scaled_array).flatten(),fmt=\"%d\", delimiter=\",\")\n",
        "\n",
        "  # Mult-Head Projection\n",
        " # Mult-Head Projection\n",
        "  mha_array = mha(ln1_array,patches_num,patch_width,patch_height,0)\n",
        "  mha_scaled_array = mha(ln1_scaled_array,patches_num,patch_width,patch_height,1)\n",
        "  import os\n",
        "  mha_output_dir = \"/content/drive/MyDrive/dataset_vision_trands/ViT/Outputs/mha/\"\n",
        "  os.makedirs(mha_output_dir, exist_ok=True)\n",
        "  np.savetxt(os.path.join(mha_output_dir, \"infered_mha.txt\"), np.array(mha_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "  np.savetxt(os.path.join(mha_output_dir, \"infered_mha_scaled.txt\"), np.array(mha_scaled_array).flatten(),fmt=\"%d\", delimiter=\",\")\n",
        "\n",
        "  mha_np_arry = np.array(mha_array).reshape(256,1,64)\n",
        "  lp_np_arry = np.array(lp_array)\n",
        "\n",
        "  # Add1\n",
        "  lp_np_arry         = np.array(lp_array)\n",
        "  lp_scaled_np_arry  = np.array(lp_array_scaled )\n",
        "  mha_np_arry        = np.array(mha_array).reshape(256,1,64)\n",
        "  mha_scaled_np_arry = np.array(mha_scaled_array).reshape(256,1,64)\n",
        "  add1_array         = add(lp_np_arry,mha_np_arry,256)\n",
        "  add1_scaled_array  = add(lp_scaled_np_arry,mha_scaled_np_arry,256)\n",
        "\n",
        "  np.savetxt(add1_infered_path, np.array(add1_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "  np.savetxt(add1_scaled_infered_path, np.array(add1_scaled_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "\n",
        "  # LN2\n",
        "  ln2_array = ln1(add1_array,patches_num,patch_width,patch_height,0)\n",
        "  ln2_scaled_array = ln1(add1_scaled_array,patches_num,patch_width,patch_height,1)\n",
        "  # np.savetxt(ln2_infered_path, np.array(ln2_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "  # np.savetxt(ln2_scaled_infered_path, np.array(ln2_scaled_array).flatten(),fmt=\"%d\", delimiter=\",\")\n",
        "\n",
        "  # MLP\n",
        "  mlp_array = mlp(ln2_array)\n",
        "  mlp_scaled_array = mlp(ln2_scaled_array,)\n",
        "  np.savetxt(mlp_infered_path, np.array(mlp_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "  np.savetxt(mlp_scaled_infered_path, np.array(mlp_scaled_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "\n",
        "  # Add2\n",
        "  mlp_np_arry         = np.array(mlp_array).reshape(256,1,64)\n",
        "  mlp_scaled_np_arry  = np.array(mlp_scaled_array).reshape(256,1,64)\n",
        "  add1_np_arry        = np.array(add1_array)\n",
        "  add1_scaled_np_arry = np.array(add1_scaled_array)\n",
        "  add2_array          = add(add1_np_arry,mlp_np_arry,256)\n",
        "  add2_scaled_array   = add(mlp_scaled_np_arry,mha_scaled_np_arry,256)\n",
        "\n",
        "  np.savetxt(add2_infered_path, np.array(add2_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "  np.savetxt(add2_scaled_infered_path, np.array(add2_scaled_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "\n",
        "  # Layer Norm3\n",
        "  ln3_array = ln1(add2_array,patches_num,patch_width,patch_height,0)\n",
        "  ln3_scaled_array = ln1(add2_scaled_array,patches_num,patch_width,patch_height,1)\n",
        "  # np.savetxt(ln3_infered_path, np.array(ln3_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "  # np.savetxt(ln3_scaled_infered_path, np.array(ln3_scaled_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "\n",
        "  # Fully Connected\n",
        "  fc_out = fc(ln3_array)\n",
        "  fc_softmax = softmax(fc_out,0)\n",
        "  print(fc_softmax [0][0])\n",
        "\n",
        "\n",
        "vit_infer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um5BAet5iVFH",
        "outputId": "94f352da-fb93-4f72-9b21-4758a0c5b13e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dense2  [-11.53631215 -14.74945451]\n",
            "[0.96132586 0.03867414]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vision Transfomer\n",
        "\n",
        "# Inference\n",
        "def vit_infer():\n",
        "  # Reading Image\n",
        "  img_data   = Image.open(dataset_path)\n",
        "\n",
        "  # Augmentation\n",
        "  augmented = data_augmentation(np.array(img_data))\n",
        "\n",
        "  # Convert to NumPy array\n",
        "  aug_img      = np.array(augmented).astype(\"uint8\")\n",
        "\n",
        "  # Patching\n",
        "  patches_num = (img_height // patch_height) * (img_width // patch_width)\n",
        "  patches     = patch(aug_img,img_width,img_height,patch_width,patch_height)\n",
        "\n",
        "  # Linear Projection\n",
        "  lp_array        = lp(patches,patches_num,patch_width,patch_height,0)\n",
        "  lp_array_scaled = lp(patches,patches_num,patch_width,patch_height,1)\n",
        "  np.savetxt(lp_infered_path, np.array(lp_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "  np.savetxt(lp_scaled_infered_path, np.array(lp_array_scaled).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "\n",
        "  # Layer Normalization\n",
        "  ln1_array = ln1(lp_array,patches_num,patch_width,patch_height,0)\n",
        "  ln1_scaled_array = ln1(lp_array_scaled,patches_num,patch_width,patch_height,1)\n",
        "  np.savetxt(ln1_infered_path, np.array(ln1_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "  np.savetxt(ln1_scaled_infered_path, np.array(ln1_scaled_array).flatten(),fmt=\"%d\", delimiter=\",\")\n",
        "\n",
        "  # Mult-Head Projection\n",
        " # Mult-Head Projection\n",
        "  mha_array = mha(ln1_array,patches_num,patch_width,patch_height,0)\n",
        "  mha_scaled_array = mha(ln1_scaled_array,patches_num,patch_width,patch_height,1)\n",
        "  np.savetxt(\"/content/drive/MyDrive/dataset_vision_trands/ViT/Outputs/mha/infered_mha.txt\", np.array(mha_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "  np.savetxt(\"/content/drive/MyDrive/dataset_vision_trands/ViT/Outputs/mha/infered_mha_scaled.txt\", np.array(mha_scaled_array).flatten(),fmt=\"%d\", delimiter=\",\")\n",
        "\n",
        "  mha_np_arry = np.array(mha_array).reshape(256,1,64)\n",
        "  lp_np_arry = np.array(lp_array)\n",
        "\n",
        "  # Add1\n",
        "  lp_np_arry         = np.array(lp_array)\n",
        "  lp_scaled_np_arry  = np.array(lp_array_scaled )\n",
        "  mha_np_arry        = np.array(mha_array).reshape(256,1,64)\n",
        "  mha_scaled_np_arry = np.array(mha_scaled_array).reshape(256,1,64)\n",
        "  add1_array         = add(lp_np_arry,mha_np_arry,256)\n",
        "  add1_scaled_array  = add(lp_scaled_np_arry,mha_scaled_np_arry,256)\n",
        "\n",
        "  np.savetxt(add1_infered_path, np.array(add1_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "  np.savetxt(add1_scaled_infered_path, np.array(add1_scaled_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "\n",
        "  # LN2\n",
        "  ln2_array = ln1(add1_array,patches_num,patch_width,patch_height,0)\n",
        "  ln2_scaled_array = ln1(add1_scaled_array,patches_num,patch_width,patch_height,1)\n",
        "  np.savetxt(ln2_infered_path, np.array(ln2_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "  np.savetxt(ln2_scaled_infered_path, np.array(ln2_scaled_array).flatten(),fmt=\"%d\", delimiter=\",\")\n",
        "\n",
        "  # MLP\n",
        "  mlp_array = mlp(ln2_array)\n",
        "  mlp_scaled_array = mlp(ln2_scaled_array,)\n",
        "  np.savetxt(mlp_infered_path, np.array(mlp_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "  np.savetxt(mlp_scaled_infered_path, np.array(mlp_scaled_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "\n",
        "  # Add2\n",
        "  mlp_np_arry         = np.array(mlp_array).reshape(256,1,64)\n",
        "  mlp_scaled_np_arry  = np.array(mlp_scaled_array).reshape(256,1,64)\n",
        "  add1_np_arry        = np.array(add1_array)\n",
        "  add1_scaled_np_arry = np.array(add1_scaled_array)\n",
        "  add2_array          = add(add1_np_arry,mlp_np_arry,256)\n",
        "  add2_scaled_array   = add(mlp_scaled_np_arry,mha_scaled_np_arry,256)\n",
        "\n",
        "  np.savetxt(add2_infered_path, np.array(add2_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "  np.savetxt(add2_scaled_infered_path, np.array(add2_scaled_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "\n",
        "  # Layer Norm3\n",
        "  ln3_array = ln1(add2_array,patches_num,patch_width,patch_height,0)\n",
        "  ln3_scaled_array = ln1(add2_scaled_array,patches_num,patch_width,patch_height,1)\n",
        "  np.savetxt(ln3_infered_path, np.array(ln3_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "  np.savetxt(ln3_scaled_infered_path, np.array(ln3_scaled_array).flatten(),fmt=\"%.6f\", delimiter=\",\")\n",
        "\n",
        "  # Fully Connected\n",
        "  fc_out = fc(ln3_array)\n",
        "  fc_softmax = softmax(fc_out,0)\n",
        "  print(fc_softmax [0][0])\n",
        "\n",
        "\n",
        "vit_infer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHMCFZPMpGoI",
        "outputId": "745f4623-e7b7-4465-a081-c051b12365f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dense2  [-11.53631215 -14.74945451]\n",
            "[0.96132586 0.03867414]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.special import erf\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"\n",
        "    Compute the GELU activation function using the error function (erf).\n",
        "\n",
        "    Args:\n",
        "        x (numpy array or float): Input value(s)\n",
        "\n",
        "    Returns:\n",
        "        numpy array or float: GELU activated value(s)\n",
        "    \"\"\"\n",
        "    return 0.5 * x * (1 + erf(x / np.sqrt(2)))\n",
        "import numpy as np\n",
        "from scipy.special import erf\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"\n",
        "    Compute the GELU activation function using the error function (erf).\n",
        "\n",
        "    Args:\n",
        "        x (numpy array or float): Input value(s)\n",
        "\n",
        "    Returns:\n",
        "        numpy array or float: GELU activated value(s)\n",
        "    \"\"\"\n",
        "    return 0.5 * x * (1 + erf(x / np.sqrt(2)))\n",
        "\n",
        "# Example usage\n",
        "x = np.array([0.450000, -0.250000, 0.650000])\n",
        "print(\"GELU Output:\", gelu(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKMAxHwZtnXY",
        "outputId": "0d67971c-7af6-4729-f1b0-a3b87536a5d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GELU Output: [ 0.30314015 -0.10032342  0.48240003]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gelu_approx(x):\n",
        "    \"\"\"\n",
        "    Compute the approximate GELU activation function using tanh.\n",
        "\n",
        "    Args:\n",
        "        x (numpy array or float): Input value(s)\n",
        "\n",
        "    Returns:\n",
        "        numpy array or float: GELU activated value(s)\n",
        "    \"\"\"\n",
        "    return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))\n",
        "\n",
        "# Example usage\n",
        "import numpy as np\n",
        "x = np.array([[2.30 ,   0.80 ,   2.20 ,   0.90],\n",
        "              [5.10 ,   1.80  ,  5.20  ,  1.90]])\n",
        "print(\"Approximate GELU Output:\", gelu_approx(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qe49rOKuky_",
        "outputId": "7f2d658a-5247-4a5d-bf8f-adb811f4f455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Approximate GELU Output: [[2.27567297 0.63043169 2.16967853 0.73422849]\n",
            " [5.09999988 1.73525953 5.19999994 1.84545123]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Coe File Generator combine 2 16-bit signed and coverts it to 2s complement\n",
        "# and generate a COE format file for Xilinx BRAM\n",
        "def coe_gen(input_file,output_file,rows,cols):\n",
        "\n",
        "  # Load Input\n",
        "  input = np.loadtxt(input_file, delimiter=\",\")\n",
        "  # Scaling\n",
        "  input_int    = np.array(input * 1024 , dtype=np.int32)\n",
        "  input_scaled = np.array(input * 1024 ,dtype=np.int32)\n",
        "  # Type Casing to Int\n",
        "  input_repacked = input_int\n",
        "  # print(len(input_int))\n",
        "  # print(len(input_repacked))\n",
        "  for i in range(0,cols):\n",
        "    for j in range(0,rows):\n",
        "      input_repacked[i*rows + j] = input_scaled[i + j *32]\n",
        "      if(i == 1 and j == 1):\n",
        "        print(input_int[i + j *cols],input_scaled[i + j *cols],input[i + j *cols])\n",
        "\n",
        "  with open(output_file, 'w+') as file:\n",
        "    # Write some text\n",
        "    file.write(\"memory_initialization_radix=16;\\n\")\n",
        "    file.write(\"memory_initialization_vector=\\n\")\n",
        "\n",
        "    for i in range(0, len(input_int), 2):\n",
        "      # Get two consecutive 16-bit values\n",
        "      value0     = np.int32(input_repacked[i])\n",
        "      value1     = np.int32(input_repacked[i+1])\n",
        "      hex_value0 = f\"{value0 & 0xFFFF:04X}\"\n",
        "      hex_value1 = f\"{value1 & 0xFFFF:04X}\"\n",
        "      combined_str = hex_value1 + hex_value0\n",
        "\n",
        "      # Write to file\n",
        "      file.write(combined_str)\n",
        "\n",
        "      # Special Case for last line\n",
        "      if(i == len(input_int) - 2):\n",
        "          file.write(\";\")\n",
        "\n",
        "      else:\n",
        "        file.write(\",\\n\")\n",
        "\n",
        "coe_gen(lp_weight_path,lp_coe_path,192,64)\n",
        "\n",
        "\n",
        "#coe_gen(lp_emb_path ,position_emb_coe_path)\n",
        "#coe_gen(lp_weight_path,lp_coe_path)\n",
        "# coe_gen(lp_bias_path,bias_coe_path)\n",
        "# coe_gen(mha_wk0_path,mha_wk0_coe_path,64,32)\n",
        "# coe_gen(mha_wv0_path,mha_wv0_coe_path,64,32)\n",
        "# coe_gen(mha_wq1_path,mha_wq1_coe_path,64,32)\n",
        "# coe_gen(mha_wk1_path,mha_wk1_coe_path,64,32)\n",
        "# coe_gen(mha_wv1_path,mha_wv1_coe_path,64,32)\n",
        "# coe_gen(mha_wqkv_path,mha_wqkv_comb_coe_path,64,32)\n",
        "# coe_gen(mha_wo_path,mha_wo_coe_path,64,32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "IsIw699jd9zd",
        "outputId": "e3eb79cb-060e-4476-fb0f-f0849ef86b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "coe_gen() missing 2 required positional arguments: 'rows' and 'cols'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1809261126.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mcoe_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlp_weight_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m192\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: coe_gen() missing 2 required positional arguments: 'rows' and 'cols'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Coe File Generator combine 2 16-bit signed and coverts it to 2s complement\n",
        "# and generate a COE format file for Xilinx BRAM\n",
        "def coe_gen_lp(input_file,output_file):\n",
        "\n",
        "  # Load Input\n",
        "  input = np.loadtxt(input_file, delimiter=\",\")\n",
        "  # Scaling\n",
        "  input_int    = np.array(input * 1024, dtype=np.int32)\n",
        "  input_scaled = np.array(input * 1024,dtype=np.int32)\n",
        "\n",
        "  with open(output_file, 'w+') as file:\n",
        "    # Write some text\n",
        "    file.write(\"memory_initialization_radix=16;\\n\")\n",
        "    file.write(\"memory_initialization_vector=\\n\")\n",
        "\n",
        "    for i in range(0, len(input_int), 2):\n",
        "      # Get two consecutive 16-bit values\n",
        "      value0     = np.int32(input_int[i])\n",
        "      value1     = np.int32(input_int[i+1])\n",
        "      hex_value0 = f\"{value0 & 0xFFFF:04X}\"\n",
        "      hex_value1 = f\"{value1 & 0xFFFF:04X}\"\n",
        "      combined_str = hex_value1 + hex_value0\n",
        "\n",
        "      # Write to file\n",
        "      file.write(combined_str)\n",
        "\n",
        "      # Special Case for last line\n",
        "      if(i == len(input_int) - 2):\n",
        "          file.write(\";\")\n",
        "\n",
        "      else:\n",
        "        file.write(\",\\n\")\n",
        "\n",
        "\n",
        "coe_gen_lp(lp_emb_path ,position_emb_coe_path)\n",
        "coe_gen_lp(lp_bias_path,bias_coe_path)"
      ],
      "metadata": {
        "id": "Qgy0uhV6IvV9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "50461c65-5cbb-4f4b-a858-8e35cc06229e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'position_emb_coe_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1504942265.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mcoe_gen_lp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlp_emb_path\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mposition_emb_coe_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mcoe_gen_lp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlp_bias_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias_coe_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'position_emb_coe_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "23i7nNvuJKhZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}