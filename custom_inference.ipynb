{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##own_innference"
      ],
      "metadata": {
        "id": "x35PSUkO6Ap-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGZGHGIF5kXi"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install tensorflow -q\n",
        "!pip install opencv-python -q\n",
        "!pip install scikit-learn -q\n",
        "!pip install keras -q\n",
        "\n",
        "# Import the libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "from PIL import Image\n",
        "from keras import layers\n",
        "from scipy.special import erf # Importing erf function for GELU\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGWRNlG1617_",
        "outputId": "cd5eb6e1-2f63-430e-ec75-89e6cb5a91ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Image and Model Parameters\n",
        "patch_height = 8\n",
        "patch_width = 8\n",
        "img_height = 128\n",
        "img_width = 128\n",
        "\n",
        "# Path for the input image\n",
        "dataset_path = \"/content/drive/MyDrive/drone_vit/drone_dataset/no drone/00000463_(5).jpg\"\n",
        "\n",
        "# Path to the directory where your TFLite weights are stored\n",
        "weights_base_path = \"/content/drive/MyDrive/drone_vit/Outputs_Quantized_Matched_Names/weights/\"\n",
        "\n",
        "# Define paths for all weight files\n",
        "lp_weight_path = weights_base_path + \"lp_weight.txt\"\n",
        "lp_emb_path = weights_base_path + \"lp_pos_emb.txt\"\n",
        "mha_wq0_path = weights_base_path + \"mha_weight_q0.txt\"\n",
        "mha_wk0_path = weights_base_path + \"mha_weight_k0.txt\"\n",
        "mha_wv0_path = weights_base_path + \"mha_weight_v0.txt\"\n",
        "mha_wq1_path = weights_base_path + \"mha_weight_q1.txt\"\n",
        "mha_wk1_path = weights_base_path + \"mha_weight_k1.txt\"\n",
        "mha_wv1_path = weights_base_path + \"mha_weight_v1.txt\"\n",
        "mha_wouput_path = weights_base_path + \"mha_weight_last.txt\"\n",
        "mlp_d0_w_path = weights_base_path + \"mlp_weight0.txt\"\n",
        "mlp_d0_b_path = weights_base_path + \"mlp_bias0.txt\"\n",
        "mlp_d1_w_path = weights_base_path + \"mlp_weight1.txt\"\n",
        "mlp_d1_b_path = weights_base_path + \"mlp_bias1.txt\"\n",
        "fc_d0_w_path = weights_base_path + \"fc_weight0.txt\"\n",
        "fc_d0_b_path = weights_base_path + \"fc_bias0.txt\"\n",
        "fc_d1_w_path = weights_base_path + \"fc_weight1.txt\"\n",
        "fc_d1_b_path = weights_base_path + \"fc_bias1.txt\"\n",
        "fc_d2_w_path = weights_base_path + \"fc_weight2.txt\"\n",
        "fc_d2_b_path = weights_base_path + \"fc_bias2.txt\"\n",
        "\n",
        "print(\"✅ All paths and parameters are set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYZW7vtZ7Fau",
        "outputId": "7b9b3837-8e39-4d53-ffc6-9046f9515076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All paths and parameters are set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = keras.Sequential([layers.Resizing(img_width, img_height)])\n",
        "def gelu_exact(x): return 0.5 * x * (1 + erf(x / np.sqrt(2)))\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "def patch(img, img_width, img_height, patch_width, patch_height):\n",
        "    patches = []\n",
        "    for h in range(0, img_height, patch_height):\n",
        "        for w in range(0, img_width, patch_width):\n",
        "            patches.append(img[h:h+patch_height, w:w+patch_width])\n",
        "    return patches\n",
        "print(\"✅ Helper functions defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVP7YZGW7MIX",
        "outputId": "ef87e9cd-cc8e-4c24-dc04-57e1a7a53191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Helper functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lp(data, patches_num):\n",
        "    weight = np.loadtxt(lp_weight_path, delimiter=\",\", dtype=np.float32).reshape(192, 64)\n",
        "    pos_emb = np.loadtxt(lp_emb_path, delimiter=\",\", dtype=np.float32).reshape(256, 64)\n",
        "    embed_img = []\n",
        "    for i in range(patches_num):\n",
        "        flattened_patch = data[i].reshape(1, 192)\n",
        "        weighted_patch = np.dot(flattened_patch, weight)\n",
        "        embed_patch = weighted_patch + pos_emb[i]\n",
        "        embed_img.append(embed_patch)\n",
        "    return embed_img\n",
        "\n",
        "def ln(data):\n",
        "    data_matrix = np.array(data).squeeze()\n",
        "    mean = np.mean(data_matrix, axis=1, keepdims=True)\n",
        "    stdev = np.std(data_matrix, axis=1, keepdims=True) + 1e-6\n",
        "    return (data_matrix - mean) / stdev\n",
        "\n",
        "def mha(data, patches_num):\n",
        "    wq0, wk0, wv0 = [np.loadtxt(p, delimiter=\",\", dtype=np.float32).reshape(64, 32) for p in [mha_wq0_path, mha_wk0_path, mha_wv0_path]]\n",
        "    wq1, wk1, wv1 = [np.loadtxt(p, delimiter=\",\", dtype=np.float32).reshape(64, 32) for p in [mha_wq1_path, mha_wk1_path, mha_wv1_path]]\n",
        "    w_out = np.loadtxt(mha_wouput_path, delimiter=\",\", dtype=np.float32).reshape(64, 64)\n",
        "\n",
        "    q0, k0, v0 = np.dot(data, wq0), np.dot(data, wk0), np.dot(data, wv0)\n",
        "    q1, k1, v1 = np.dot(data, wq1), np.dot(data, wk1), np.dot(data, wv1)\n",
        "\n",
        "    attention0 = softmax(np.dot(q0, k0.T) / np.sqrt(k0.shape[-1]))\n",
        "    head0 = np.dot(attention0, v0)\n",
        "    attention1 = softmax(np.dot(q1, k1.T) / np.sqrt(k1.shape[-1]))\n",
        "    head1 = np.dot(attention1, v1)\n",
        "\n",
        "    concatenated = np.hstack((head0, head1))\n",
        "    return np.dot(concatenated, w_out)\n",
        "\n",
        "def add(input1, input2):\n",
        "    arr1 = np.array(input1).squeeze()\n",
        "    arr2 = np.array(input2).squeeze()\n",
        "    return np.add(arr1, arr2)\n",
        "\n",
        "def mlp(data):\n",
        "    w0, b0 = np.loadtxt(mlp_d0_w_path, delimiter=\",\", dtype=np.float32).reshape(64, 128), np.loadtxt(mlp_d0_b_path, delimiter=\",\", dtype=np.float32).reshape(1, 128)\n",
        "    w1, b1 = np.loadtxt(mlp_d1_w_path, delimiter=\",\", dtype=np.float32).reshape(128, 64), np.loadtxt(mlp_d1_b_path, delimiter=\",\", dtype=np.float32).reshape(1, 64)\n",
        "    d0 = gelu_exact(np.dot(data, w0) + b0)\n",
        "    d1 = gelu_exact(np.dot(d0, w1) + b1)\n",
        "    return d1\n",
        "\n",
        "def fc(data):\n",
        "    flat_data = data.flatten().reshape(1, -1)\n",
        "    w0, b0 = np.loadtxt(fc_d0_w_path, delimiter=\",\", dtype=np.float32).reshape(16384, 2048), np.loadtxt(fc_d0_b_path, delimiter=\",\", dtype=np.float32).reshape(1, 2048)\n",
        "    w1, b1 = np.loadtxt(fc_d1_w_path, delimiter=\",\", dtype=np.float32).reshape(2048, 1024), np.loadtxt(fc_d1_b_path, delimiter=\",\", dtype=np.float32).reshape(1, 1024)\n",
        "    w2, b2 = np.loadtxt(fc_d2_w_path, delimiter=\",\", dtype=np.float32).reshape(1024, 2), np.loadtxt(fc_d2_b_path, delimiter=\",\", dtype=np.float32).reshape(1, 2)\n",
        "    d0 = gelu_exact(np.dot(flat_data, w0) + b0)\n",
        "    d1 = gelu_exact(np.dot(d0, w1) + b1)\n",
        "    d2 = np.dot(d1, w2) + b2\n",
        "    return d2\n",
        "\n",
        "print(\"✅ Core layer simulation functions defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMAYbgmu7OdG",
        "outputId": "480ab36f-6f0f-4a84-947a-46e18f2bd9a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Core layer simulation functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vit_infer_simulation():\n",
        "    img_data = Image.open(dataset_path)\n",
        "    augmented = data_augmentation(np.array(img_data))\n",
        "    aug_img = np.array(augmented).astype(\"float32\") / 255.0\n",
        "    patches_num = (img_height // patch_height)**2\n",
        "    patches = patch(aug_img, img_width, img_height, patch_width, patch_height)\n",
        "\n",
        "    lp_array = lp(patches, patches_num)\n",
        "    ln1_array = ln(lp_array)\n",
        "    mha_array = mha(ln1_array, patches_num)\n",
        "    add1_array = add(lp_array, mha_array)\n",
        "    ln2_array = ln(add1_array)\n",
        "    mlp_array = mlp(ln2_array)\n",
        "    add2_array = add(add1_array, mlp_array)\n",
        "    ln3_array = ln(add2_array)\n",
        "    fc_out = fc(ln3_array)\n",
        "    fc_softmax = softmax(fc_out)\n",
        "\n",
        "    predicted_class = np.argmax(fc_softmax)\n",
        "\n",
        "    print(\"\\n--- Simulation Inference (Modified NumPy Script) ---\")\n",
        "    print(f\"Image Path: {dataset_path}\")\n",
        "    print(f\"Simulated Probabilities: {fc_softmax[0]}\")\n",
        "\n",
        "    print(f\"Simulated Predicted Class: {'no drone' if predicted_class == 0 else 'drone'}\")\n",
        "\n",
        "print(\"✅ Main inference function defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGAepfgW7SDm",
        "outputId": "3ded8e7a-360b-444f-90c5-3cd9887ecf56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Main inference function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the complete inference simulation\n",
        "vit_infer_simulation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkI1zmdy7UvO",
        "outputId": "8dfc1a40-85db-4aa3-e009-ec0ffbb3a6f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Simulation Inference (Modified NumPy Script) ---\n",
            "Image Path: /content/drive/MyDrive/drone_vit/drone_dataset/no drone/00000463_(5).jpg\n",
            "Simulated Probabilities: [1. 0.]\n",
            "Simulated Predicted Class: no drone\n"
          ]
        }
      ]
    }
  ]
}